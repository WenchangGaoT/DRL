{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY_MseAx-NEn"
      },
      "source": [
        "# Chapter 2: REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.distributions\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import gym"
      ],
      "metadata": {
        "id": "fIyLSGdAFwME"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygame\n",
        "!apt-get install python-opengl -y\n",
        "\n",
        "!apt install xvfb -y\n",
        "\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!pip install piglet\n",
        "\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "VmdSjr83pWDf",
        "outputId": "26a9c352-57e8-4115-df53-65d2c827336b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.7/dist-packages (2.1.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.11).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: piglet-templates in /usr/local/lib/python3.7/dist-packages (from piglet) (1.3.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (3.0.9)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (22.1.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.0.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (0.37.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscretePolicy(nn.Module):\n",
        "  \"\"\"\n",
        "  REINFORCE policy that generates discrete\n",
        "  probability distributions of action wrt state\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, in_dim=10, out_dim=10):\n",
        "    super(DiscretePolicy, self).__init__()\n",
        "    self.model = nn.Sequential(*[\n",
        "        nn.Linear(in_dim, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, out_dim),\n",
        "        nn.Softmax()\n",
        "    ])\n",
        "    self.onpolicy_reset()\n",
        "    \n",
        "  def onpolicy_reset(self):\n",
        "    self.log_probs = []\n",
        "    self.rewards = []\n",
        "  \n",
        "  def forward(self, x):\n",
        "    pdparam = self.model(x)\n",
        "    return pdparam\n",
        "  \n",
        "  def action(self, state):\n",
        "    x = torch.from_numpy(state.astype(np.float32))\n",
        "    pdparam = self.forward(x) # Generate action pd wrt state\n",
        "    pd = torch.distributions.Categorical(logits=pdparam) \n",
        "    action = pd.sample() # Choose action according to generated pd\n",
        "    log_prob = pd.log_prob(action) \n",
        "    self.log_probs.append(log_prob) # Record log prob of this action \n",
        "    return action.item()"
      ],
      "metadata": {
        "id": "A1dGDe3gGecp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(policy, optimizer, gamma=0.99):\n",
        "  T = len(policy.rewards)\n",
        "  rets = np.empty(T, dtype=np.float32)\n",
        "  future_ret = 0.\n",
        "  for t in reversed(range(T)):\n",
        "    future_ret = policy.rewards[t] + gamma*future_ret\n",
        "    rets[t] = future_ret\n",
        "  rets = torch.tensor(rets)\n",
        "  log_probs = torch.stack(policy.log_probs)\n",
        "  loss = -log_probs*rets \n",
        "  loss = torch.sum(loss)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss.item()"
      ],
      "metadata": {
        "id": "e_FLfZNDlkK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  env = gym.make('CartPole-v0')\n",
        "  env.reset()\n",
        "  in_dim = env.observation_space.shape[0]\n",
        "  out_dim = env.action_space.n\n",
        "  pi = DiscretePolicy(in_dim, out_dim)\n",
        "  optimizer = optim.Adam(pi.parameters(), lr=0.0001)\n",
        "  for epi in range(3000):\n",
        "    state = env.reset()\n",
        "    for t in range(200):\n",
        "      action = pi.action(state)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "      pi.rewards.append(reward)\n",
        "      if done: break\n",
        "    loss = train(pi, optimizer)\n",
        "    total_reward = sum(pi.rewards)\n",
        "    solved = total_reward>195\n",
        "    pi.onpolicy_reset()\n",
        "    if epi%100 == 0: \n",
        "      print(f'Episode {epi}, loss: {loss}, total reward: {total_reward}')"
      ],
      "metadata": {
        "id": "1yTly1u-nhuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "metadata": {
        "id": "Jo-PR-QlowSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContinuousPolicy(nn.Module):\n",
        "  \"\"\"\n",
        "  REINFORCE policy that generates continuous probability distributions\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, in_dim):\n",
        "    super(ContinuousPolicy, self).__init__()\n",
        "    self.model = nn.Sequential(*[\n",
        "        nn.Linear(in_dim, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 2), # mean and variance as pdparam for normal distribution\n",
        "    ])\n",
        "    self.onpolicy_reset()\n",
        "  \n",
        "  def forward(self, state):\n",
        "    pdparam = self.model(state)\n",
        "    return pdparam\n",
        "\n",
        "  def onpolicy_reset(self):\n",
        "    self.log_probs = []\n",
        "    self.rewards = []\n",
        "  \n",
        "  def action(self, state):\n",
        "    if state.dtype != torch.float32:\n",
        "      state = torch.from_numpy(state.astype(np.float32))\n",
        "    pdparam = self.forward(state)\n",
        "    pd = torch.distributions.Normal(loc=pdparam[0], scale=pdparam[1])\n",
        "    action = pd.sample()\n",
        "    log_prob = pd.log_prob(action)\n",
        "    self.log_probs.append(log_prob)\n",
        "    return action.item()\n",
        "  "
      ],
      "metadata": {
        "id": "rNdL8zaLoz-T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0FjLnOPwRGYT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}